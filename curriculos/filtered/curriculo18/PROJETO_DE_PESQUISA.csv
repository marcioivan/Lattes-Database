ANO_INICIO,ANO_FIM,NOME,SITUACAO,NATUREZA,DESCRICAO
2011,2013,Acelerando sistemas de recuperação de imagens por conteúdo (CBIR) com GPUs e APUs,CONCLUIDO,PESQUISA,"Neste projeto, investigamos e desenvolvemos técnicas para a implementação e execução eficiente de algoritmos de ""re-ranking"" para classificação de imagens em arquiteturas paralelas heterogêneas, incluindo GPUs e APUs."
2011,2013,Algoritmos e suporte em hardware para projeto de máquinas virtuais eficientes,CONCLUIDO,PESQUISA,"Neste projeto, investigamos algoritmos e técnicas em hardware para auxiliar a implementação de máquinas virtuais eficientes. Máquinas virtuais são programas de computador que emulam uma interface para execução de outros programas, compilados para a interface sendo emulada. Esta tecnologia está presente em diversos sistemas computacionais e é utilizada desde o suporte à linguagens de programação de alto nível, como na máquina virtual Java, até a implementação de processadores com projeto integrado de hardware e software, como é o caso processador Efficeon da Transmeta. A maioria das máquinas virtuais existentes na literatura utiliza técnicas de emulação similares, como interpretação e tradução dinâmica de binários. De fato, as duas técnicas são muitas vezes utilizadas de forma colaborativa, em uma mesma máquina virtual, para acelerar o processo de emulação e tornar a máquina virtual eficiente. Dada a importância do processo de emulação nas máquinas virtuais, o objetivo deste projeto de pesquisa é investigar algoritmos e técnicas em hardware para acelerar o processo de emulação em máquinas virtuais."
2012,2014,Técnicas de Formação e Identificação de Regiões Quentes e Paralelização de Laços usando Software Pipeline Desacoplado,CONCLUIDO,PESQUISA,Este projeto tem por objetivo desenvolver novas técnicas na área de paralelização de regiões quentes em máquinas virtuais.
2012,2014,"ExaMind: Busca, Classificação e Visualização de Dados Complexos em Grande Escala Utilizando Processamento de Alto Desempenho",CONCLUIDO,PESQUISA,"At the avant-garde of scienti c and industrial computing, there are problems involving very large-scale streams and collections of complex data. Multimedia, multimodal, spatial-temporal or involving social networks, these data are beyond the models created for handling structured and textual information. Thus, two dimensions appear as immediate challenges: the volume of data and their complexity. At the front end of large-scale and high complexity, it is insu cient to work in isolation with algorithmic solutions or with execution environment improvements. Rather, it is necessary to conceive and synergistically combine those two lines of work. New massively parallel architectures, derived from the GPUs (graphics processing units) and APUs (advanced processing units), promise to disrupt the high-performance computing market. However, due to the large di erence between those architectures and those based on conventional CPUs, the concretization of their potential is far from trivial, involving intricate compromises ranging from algorithm design to online dynamic adjustments made at runtime. Advancing the state of the art at this vanguard is the goal of this ambitious project, which brings together a team with complementary skills: on one hand, in the elds of machine learning, multimedia search and classi cation, and processing of other complex data and streams; on the other hand, in the elds of parallel computing, high performance computing and new computing architectures. The versatility and relevance of the techniques studied will be demonstrated in bold applications of high impact: Information Retrieval, Multimedia Classi cation, and Visualization of Complex Data. The project encompasses data analysis and data mining, which have become a key activity for many organizations. It falls within the scope of two of the Grand Challenges in Computing: Information Management in Large Volumes of Distributed Multimedia Data, and Impacts on Computing of the Transition from"
2012,2016,Content-Based Image Retrieval (CBIR) on APU Clouds,CONCLUIDO,PESQUISA,"The usefulness of re-ranking approaches for CBIR systems depends not only on the
effectiveness, but also on the efficiency and scalability. While the effectiveness is related to the quality of retrieved images, the efficiency refers to the time spent to obtain the results.

Scalability considers the system capability of handling growing image collections. Although the effectiveness has been the focus of various recent works, dealing with those three requirements at the same time is essential in real-world applications. Our recent results (supported by AMD Research), show that image re-ranking algorithms can be accelerated using GPGPUs and has great potential to explore the features of the APUs architecture. However, for very large image collections, it is also important to ensure that these algorithms can scale well with the growth of the data set.

Aiming at computing the relationship among images, re-ranking algorithms often consider all the distances among images of a given dataset, which represent a large computational effort, hindering its use in searching services that deal with real-world image collections.

However, our recent findings indicate that re-ranking strategies can still improve the
effectiveness of the results even if only a small subset of the ranked list is taken into account when performing the re-ranking procedure. Such a observation allowed us to design image re-ranking algorithms with lower complexity. The next steps will be dedicated to make them scalable and, hence, well suited to large datasets."
2013,2015,Modelagem de consumo de energia em dispositivos móveis baseados em processadores ARM,CONCLUIDO,PESQUISA,"O principal objetivo neste projeto é entender como a energia é consumida pelos componentes de hardware como CPU, GPU, memória, e dispositivos periféricos, levando em consideração todas as camadas da plataforma do software. Os resultados esperados incluem modelos de consumo de energia, uma metodologia eficiente para gerar estes modelos e medidas de consumo de energia para sistemas modernos baseados nos processadores ARM, considerando-se diferentes modos de consumo de energia do processador."
2013,2015,Algoritmos e suporte em hardware para o projeto da máquina virtual Open-ISA VM,CONCLUIDO,PESQUISA,"Open-ISA é uma arquitetura de computador que está sendo desenvolvida no Laboratório de Sistemas de Computação da Unicamp como parte de uma estratégia para reduzir o déficit da balança comercial brasileira no setor de eletroeletrônicos. O seu desenvolvimento em conjunto com a máquina virtual Open-ISA VM permitirá que sistemas complexos, incluindo sistemas operacionais, ferramentas de produtividade e científicas sejam executados de forma transparente em diferentes arquiteturas, eliminando a vínculo do software nacional com as arquiteturas proprietárias de empresas estrangeiras.

Este projeto visa investigar o estado da arte e propor novos algoritmos e suporte em hardware para implementar a máquina virtual Open-ISA VM de forma que as aplica- ções compiladas para Open-ISA possam ser emuladas de forma eficiente em arquiteturas atuais."
2011,2015,Análise Elastoplástica de Poços e Estudo do Fraturamento Hidráulico em Reservatórios Carbonáticos,CONCLUIDO,PESQUISA,"O projeto consiste na pesquisa e desenvolvimento de técnicas para simulação numérica de fratura plana em meios heterogêneos com otimização para computadores paralelos. Nesse escopo, pretende-se realizar a simulação numérica do estado de tensão em torno do poço utilizando modelos elastoplásticos, a caracterização do comportamento não linear de rochas carbonáticas e a simulação de problemas multifísicos."
2014,2016,Algorithms and techniques to accelerate numerical simulation in the cloud,CONCLUIDO,PESQUISA,"Numerical computer simulation has become an important tool for the design of engineering products and services not only in the academia but also in high technology industries, such as aircraft, oil exploration, energy and others. 

In order to provide objective confidence levels in quantitative information obtained from numerical predictions it is important to account for the uncertainties related to the inputs of a computation attempting to represent a physical system. In this sense, stochastic analysis may be applied to measure the impact of the input parameters uncertainties on output data. However, this kind of analysis generally requires several hundreds or thousands of repeated simulations with a proper selection of the input values to provide reasonable confidence levels. These simulations, in turn, may consume a huge amount of computing power to be performed in a reasonable time frame. 

To acquire and manage a computing system with such computing power is not always feasible due to ownership cost and setup timing constraints. Moreover, the demand for resources when running simulation experiments generally varies with time leading to underutilization of the system when demand is low. Cloud computing, on the other hand, provides computing resources to consumers as a utility - consumers can access the IT resources easily and pay only for what they use. The goal of this project is to investigate algorithms and techniques for numerical computer simulation and stochastic analysis in the cloud."
2014,NULL,DeepEyes: Visual Computing and Machine Intelligence Techniques for Digital Forensics and Electronic Surveillance,EM_ANDAMENTO,PESQUISA,NULL
2014,NULL,Integração de computação heterogênea e computação na nuvem para solução de problemas de engenharia e ciência,EM_ANDAMENTO,PESQUISA,"A evolução das técnicas de simulação numérica permitiu a redução do tempo e do custo de ensaios experimentais utilizados para o desenvolvimento de técnicas e produtos em diversos segmentos da indústria. Este tipo de simulação envolve um grande número de cálculos e requer o uso de computadores de alto desempenho, o que torna seu uso restrito a grupos que possuam acesso a este tipo de sistema. O surgimento do modelo de computação na nuvem aliado ao desenvolvimento de aceleradores de baixo custo, como GPUs, disponibilizou um grande poder computacional para usuários que não possuem acesso a computadores de alto desempenho. Entretanto, a programação e aceleração de código nestes sistemas ainda é um desafio. Dessa forma, este projeto visa desenvolver diretrizes e técnicas para viabilizar a aceleração de simulações numéricas com recursos de nuvem computacional e aceleradores."
2016,NULL,"Desenvolvimento e aplicação prática de técnicas de processamento, regularização e imageamento baseadas na tecnologia 3D HPC CRS no domínio pré-empilhado.",EM_ANDAMENTO,PESQUISA,"O presente projeto tem como objetivo a generalização das técnicas de tipo Common Reflection Surface (CRS), desenvolvidas em projeto ""Algoritmos de processamento e Imageamento Sísmico com Ênfase em Reservatórios Carbonáticos"" no domínio pós-empilhado ou zero-offset (ZO), para o domínio pré-empilhado, bem como sua aplicação prática a diversos problemas importantes do imageamento sísmico. Tais problemas incluem a Regularização 5D, Migração CRP (Common Reflection Point) 3D, Tomografias NIP (Normal Incidence Point), IIP (Image Incident Point) e estereo-tomografia, bem como estudos sobre anisotropia. Além disso, essas técnicas serão desenvolvidas utilizando uma plataforma computacional (SeisHPC). Essa plataforma garante a escalabilidade do desempenho dos programas que a utilizarão, potencializando o uso intensivo de computação juntamente com técnicas de tolerância a falhas que permitem o processamento de dados sísmicos reais de grande porte (ordem de Terabytes). A vigência do projeto é de 3 anos a partir do seu início, em fevereiro de 2016."
2012,2016,Algoritmos de processamento e Imageamento Sísmico com Ênfase em Reservatórios Carbonáticos,CONCLUIDO,PESQUISA,"O presente projeto tem como principal objetivo a pesquisa e o desenvolvimento de algoritmos de processamento e imageamento de dados sísmicos ligados a reservatórios carbonáticos. Esses algoritmos decorrem de um elenco de técnicas recentes de processamento de sinais, muitas delas ainda pouco exploradas em geofísica, a serem aplicadas em sinergia com o método CRS de imageamento sísmico. O projeto se insere na proposta do Sistema de Capacitação, Ciência e Tecnologia em Carbonatos (SCTC) para o estudo e exploração em rochas sedimentares carbonáticas. O projeto está centrado na associação sinergética entre: (a) O método Common Reflection Surface (CRS) de imageamento sísmico; (b) As modernas técnicas de processamento de sinais, muitas delas oriundas de outras aplicações, tais como imagens médicas, radar, sonar e telecomunicações, onde se mostraram extremamente poderosas. A vigência do projeto é de 4 anos a partir do seu início em julho de 2012."
2011,2012,Accelerating content-based image retrieval (CBIR) systems with GPUs and APUs,CONCLUIDO,PESQUISA,"This project aims at showing that, content-based imaging benefits from APU speedups. The overall objective is to characterize the class of CBIR algorithms with respect to their adequacy to the APU architecture; provide parallelization approach intended at achieving high execution efficiency of CBIR algorithms on APUs; and Provide algorithmic and microarchitectural insights to inform the efficient implementation of CBIR on existing and future APU architectures."
2002,2007,ChameLeon - Especialização de Arquiteturas para Sistemas Dedicados,CONCLUIDO,PESQUISA,Projeto de arquiteturas de computadores com suporte a reconfiguração.
2016,NULL,SeisHPC: Plataforma de desenvolvimento para aplicações sísmicas e geofísicas de alto desempenho,EM_ANDAMENTO,PESQUISA,"Atualmente na indústria de Óleo e Gás, diversos problemas reais de imageamento sísmico apresentam um grande desafio em comum, o uso intenso de recursos computacionais de alto desempenho. Entre os problemas mencionados podemos citar as técnicas de Regularização 5D e Migração CRP (Common Reflection Point) 3D. Estas técnicas têm como objetivo, construir imagens cada vez mais precisas do subsolo de maneira a encontrar recursos naturais localizados em regiões desafiadoras.

O principal objetivo da plataforma SeisHPC é garantir a escalabilidade do desempenho dos programas que a utilizarão, potencializando o uso intensivo de computação nas CPUs e aceleradores (GPUs, Intel Xeon PHI, etc), juntamente com técnicas de tolerância a falhas visando diminuir significativamente o número de horas de processamento desperdiçados devido a diversos tipos falhas intermitentes."
2017,NULL,Accelerating the CRS seismic processing method with GPUs,EM_ANDAMENTO,PESQUISA,"Exploration geophysics is extensively applied by the oil industry to better understand the subsurface of the earth, thus rising the chances of finding reservoirs of fossil fuels and minerals. This is a branch of geophysics that aims to characterize the subsurface by conducting measurements at Earth?s surface (Land or Sea).

During seismic data acquisition, a source generates waves that propagates through the water and reflects, diffracts and refracts into the subsurface layers. Some of the reflected waves returns to the surface and meets hydrophones, which record the data. Once recorded, the seismic data is later processed to improve the signal-to-noise ratio and to construct images that helps geologists to understand the meaningful features of the subsurface. The Common Reflection Surface (CRS) algorithm (Mann et al., 1999; Jäger et al., 2001) is a generalized version of one of the key algorithms used in exploration geophysics to improve the signal-to-noise ratio, the Common Midpoint (CMP) method (Mayne, 1962).

The CRS method takes data collected in the field (after preprocessing) and generates a so called Zero-offset Stacked section, along with some properties of the subsurface structures, such as velocity, dip angles and curvatures. To accomplish this task, the method solves an optimization problem in which a coherence metric mult be computed for several combinations of parameters A, B and C, which requires a huge amount of computation. Moreover, this computation exposes a high degree of data locality, which makes it CPU bound. These properties are typically prerequisites for exploring the full potential of GPUs.

Since drilling is a very expensive task, with prices around hundreds of millions of dollars, investing in exploration geophysics is important to reduce uncertainties, saving a lot of manpower and resources.

The main objectives of this project are: a) Quantify the performance benefits when accelerating the CRS method with GPUs; and b) Understand the productivity and code performance trade-offs when using OpenACC and CUDA to accelerate seismic processing code."
2015,NULL,Exploração de Paralelismo em Hardware e Software (PROCAD 2966/2014),EM_ANDAMENTO,PESQUISA,"A Lei de Moore, que prevê o dobro do número de transistores por circuito integrado a cada 2 anos, j a não pode ser aproveitada como no passado, inviabilizando o desenvolvimento de processadores maiores e com frequência sempre crescente. Desde o in ício deste século, por causa da barreira térmica, os fabricantes de processadores focaram seus esforços no desenvolvimento de arquiteturas multicore, evoluindo pouco na execução de código sequencial e forçando o desenvolvimento de versões paralelas de aplicativos. Embora este esforço esteja mostrando algum resultado, com a criação de novas versões de programas capazes de explorar o paralelismo dos processadores modernos, não há como negar que ainda existe muito trabalho a ser feito nesta direção e que nem todos os programas terão implementações paralelas eficientes. Por outro lado, qualquer nova técnica que melhore o desempenho de aplicações sequenciais também melhorará o desempenho de suas versões paralelas. O foco deste projeto é estudar técnicas de exploração de paralelismo, tanto em software quanto em hardware, permitindo a execução eficiente de programas em processadores modernos. Estamos interessados em melhorias arquiteturais, dentro dos cores, em novas formas de interligar estes cores, numa maior quantidade deles dentro de um circuito integrado, na utilização de aceleradores como GPGPU e em composições de múltiplos computadores na forma de clusters ou nuvem. Para cada uma destas configurações de hardware, também serão necessárias pesquisas e inovações na área de software"
2015,NULL,Technologies for smart interaction with Mods and IoT devices,EM_ANDAMENTO,PESQUISA,NULL
2015,NULL,Análise Estática para Detecção de Erros de Energia,EM_ANDAMENTO,PESQUISA,NULL
